def deduplicate_summaries(drive_service, sheets_service):
    config.logger.info("üöÄ Starting deduplicate_summaries()")
    start_time = time.time()
    MAX_RUNTIME = 5 * 60  # 5 minutes in seconds

    if not helpers.try_lock_folder(config.SUMMARY_FOLDER_NAME):
        return

    try:
        summary_folder_id = google_api.get_or_create_subfolder(drive_service, config.DJ_SETS, config.SUMMARY_FOLDER_NAME)
        files = google_api.list_files_in_folder(drive_service, summary_folder_id)

        for file in files:
            elapsed = time.time() - start_time
            if elapsed > MAX_RUNTIME:
                config.logger.info("‚è≥ Time limit reached. Exiting.")
                break

            name = file["name"]
            mime_type = file["mimeType"]
            match = re.match(r"^_pending_(\d{4}) Summary$", name)
            if not match or mime_type != "application/vnd.google-apps.spreadsheet":
                continue

            try:
                config.logger.info(f"üìÑ Processing: {name}")
                spreadsheet_id = file["id"]
                # Get sheet metadata to find first sheet ID and name
                spreadsheet = (
                    sheets_service.spreadsheets()
                    .get(spreadsheetId=spreadsheet_id, fields="sheets(properties(sheetId,title))")
                    .execute()
                )
                sheets = spreadsheet.get("sheets", [])
                if not sheets:
                    config.logger.warning(f"‚ö†Ô∏è No sheets found in {name}")
                    continue
                first_sheet = sheets[0]
                sheet_id = first_sheet["properties"]["sheetId"]
                sheet_name = first_sheet["properties"]["title"]

                data = google_api.get_sheet_values(spreadsheet_id, sheet_name)
                if len(data) < 2:
                    config.logger.warning(f"‚ö†Ô∏è Skipping empty or header-only file: {name}")
                    continue

                header = data[0]
                rows = [row for row in data[1:] if "".join(row).strip() != ""]

                dedup_result = deduplicate_rows_with_soft_match(
                    [{"header": header, "rows": rows}], header
                )
                headers = dedup_result["headers"]
                rows_with_meta = dedup_result["rowsWithMeta"]
                final_header = headers + ["Count"]
                try:
                    title_index = headers.index("Title")
                except ValueError:
                    title_index = -1
                try:
                    comment_index = headers.index("Comment")
                except ValueError:
                    comment_index = -1

                with_count = [r for r in rows_with_meta if r["count"] > 0]

                soft_matches = sorted(
                    [r for r in with_count if r.get("match") == "soft"],
                    key=lambda r: str(r["data"][title_index]) if title_index >= 0 else "",
                )
                others = sorted(
                    [r for r in with_count if r.get("match") != "soft"],
                    key=lambda r: str(r["data"][title_index]) if title_index >= 0 else "",
                )

                final_rows = [final_header]
                backgrounds = [["#ffffff" for _ in final_header]]

                def should_exclude(comment):
                    lc = (comment or "").lower()
                    return "routine |" in lc or "the open 2024" in lc or "fx |" in lc

                def add_rows(lst):
                    for item in lst:
                        data_row = item["data"]
                        count = item["count"]
                        match_type = item.get("match", "")
                        comment = data_row[comment_index] if comment_index >= 0 else ""
                        if should_exclude(comment):
                            continue
                        final_rows.append(data_row + [count])
                        bg = ["#ffffff"] * len(final_header)
                        if match_type == "soft":
                            for i in range(len(headers)):
                                bg[i] = "#fff3b0"
                        backgrounds.append(bg)

                add_rows(soft_matches)
                add_rows(others)

                # Clear sheet
                google_api.clear_sheet(sheets_service, spreadsheet_id, sheet_id)

                # Update values
                google_api.update_sheet_values(spreadsheet_id, sheet_name, final_rows)

                # Apply formatting
                google_api.set_sheet_formatting(
                    spreadsheet_id=spreadsheet_id,
                    sheet_id=sheet_id,
                    header_row_count=1,
                    total_rows=len(final_rows),
                    total_cols=len(final_rows[0]),
                    backgrounds=backgrounds,
                )

                # Rename file
                new_name = f"{match.group(1)} Summary"
                drive_service.files().update(
                    fileId=spreadsheet_id, body={"name": new_name}
                ).execute()
                config.logger.info(f"‚úèÔ∏è Renamed to: {new_name}")

                time.sleep(10)  # Sleep 10 seconds
                config.logger.info(f"‚úÖ Deduplicated: {new_name}")

            except Exception as e:
                config.logger.error(f"‚ùå Error deduplicating {name}: {e}")

    finally:
        helpers.release_folder_lock(config.SUMMARY_FOLDER_NAME)
        config.logger.info("üèÅ deduplicate_summaries complete.")


def deduplicate_rows_with_soft_match(sheet_data, all_headers):
    dedup_fields = ["Title", "Remix", "Artist", "Comment", "Genre", "Year", "BPM"]
    desired_order = ["Title", "Remix", "Artist", "Comment", "Genre", "Year", "BPM", "Length"]

    lower_header_map = {h.lower(): h for h in all_headers}

    headers = [lower_header_map[h.lower()] for h in desired_order if h.lower() in lower_header_map]

    primary_fields = [f for f in ["Title", "Remix", "Artist"] if f in headers]
    secondary_fields = [f for f in dedup_fields if f not in primary_fields and f in headers]

    primary_indices = [{"field": f, "index": headers.index(f)} for f in primary_fields]
    secondary_indices = [{"field": f, "index": headers.index(f)} for f in secondary_fields]

    deduped_rows = []

    for sheet in sheet_data:
        header = sheet["header"]
        rows = sheet["rows"]
        header_map = {h.lower(): i for i, h in enumerate(header)}

        for row in rows:
            aligned = [
                row[header_map.get(h.lower(), "")] if header_map.get(h.lower(), "") != "" else ""
                for h in headers
            ]
            # Fix for possible missing keys:
            aligned = []
            for h in headers:
                idx = header_map.get(h.lower())
                if idx is not None and idx < len(row):
                    aligned.append(row[idx])
                else:
                    aligned.append("")

            matched = False
            for group in deduped_rows:
                other = group["data"]
                matches_all_primary = all(
                    aligned[p["index"]] == other[p["index"]] for p in primary_indices
                )
                if not matches_all_primary:
                    continue
                secondary_match = all(
                    (
                        not aligned[s["index"]]
                        or not other[s["index"]]
                        or aligned[s["index"]] == other[s["index"]]
                    )
                    for s in secondary_indices
                )
                if secondary_match:
                    group["count"] += 1
                    matched = True
                    break
            if not matched:
                deduped_rows.append({"data": aligned, "count": 1, "match": ""})

    # Soft match flagging
    all_indices = primary_indices + secondary_indices
    for i in range(len(deduped_rows)):
        for j in range(i + 1, len(deduped_rows)):
            a = deduped_rows[i]
            b = deduped_rows[j]
            shared = helpers.get_shared_filled_fields(a["data"], b["data"], all_indices)
            score = helpers.get_dedup_match_score(a["data"], b["data"], all_indices)

            all_similar = all(
                (
                    not str(a["data"][idx["index"]])
                    or not str(b["data"][idx["index"]])
                    or helpers.string_similarity(
                        str(a["data"][idx["index"]]), str(b["data"][idx["index"]])
                    )
                    >= 0.5
                    or helpers.clean_title(str(a["data"][idx["index"]]))
                    == helpers.clean_title(str(b["data"][idx["index"]]))
                )
                for idx in all_indices
            )

            required_shared = 2 if len(primary_indices) >= 3 else 1
            if score >= 0.5 and shared >= required_shared and all_similar:
                if not a["match"]:
                    a["match"] = "soft"
                if not b["match"]:
                    b["match"] = "soft"

    return {"headers": headers, "rowsWithMeta": deduped_rows}



def deduplicate_complete_summary():
    config.logger.info("üöÄ Starting deduplicate_complete_summary()")

    if not helpers.try_lock_folder(config.SUMMARY_FOLDER_NAME):
        return

    try:
        parent_folder_id = config.DJ_SETS
        summary_folder_id = helpers.get_or_create_subfolder(parent_folder_id, config.SUMMARY_FOLDER_NAME)
        files = google_api.list_files_in_folder(summary_folder_id)

        file = None
        for f in files:
            if f["name"].lower() == "_pending_complete summary":
                file = f
                break

        if not file:
            config.logger.warning("‚ö†Ô∏è _pending_Complete Summary file not found.")
            return

        config.logger.info("üìÑ Processing: _pending_Complete Summary")
        spreadsheet_id = file["id"]

        spreadsheet = (
            sheets_service.spreadsheets()
            .get(spreadsheetId=spreadsheet_id, fields="sheets(properties(sheetId,title))")
            .execute()
        )
        sheets = spreadsheet.get("sheets", [])
        if not sheets:
            config.logger.warning("‚ö†Ô∏è No sheets found in _pending_Complete Summary")
            return
        first_sheet = sheets[0]
        sheet_id = first_sheet["properties"]["sheetId"]
        sheet_name = first_sheet["properties"]["title"]

        data = get_sheet_values(spreadsheet_id, sheet_name)
        if len(data) < 2:
            config.logger.warning("‚ö†Ô∏è Skipping empty or header-only Complete Summary")
            return

        header = data[0]
        rows = [row for row in data[1:] if "".join(row).strip() != ""]

        dedup_result = deduplicate_rows_with_soft_match_complete_summary(
            [{"header": header, "rows": rows}]
        )
        headers = dedup_result["headers"]
        rows_with_meta = dedup_result["rowsWithMeta"]

        title_index = next((i for i, h in enumerate(headers) if h.lower() == "title"), -1)
        comment_index = next((i for i, h in enumerate(headers) if h.lower() == "comment"), -1)

        with_count = [r for r in rows_with_meta if r["count"] > 0]

        def should_remove(comment):
            c = (comment or "").lower()
            return "routine |" in c or "the open 2024" in c or "fx |" in c

        soft_matches = sorted(
            [
                r
                for r in with_count
                if r.get("match") == "soft"
                and not should_remove(r["data"][comment_index] if comment_index >= 0 else "")
            ],
            key=lambda r: str(r["data"][title_index]) if title_index >= 0 else "",
        )
        others = sorted(
            [
                r
                for r in with_count
                if r.get("match") != "soft"
                and not should_remove(r["data"][comment_index] if comment_index >= 0 else "")
            ],
            key=lambda r: str(r["data"][title_index]) if title_index >= 0 else "",
        )

        final_rows = [headers]
        backgrounds = [["#ffffff" for _ in headers]]

        # Year columns detection (4 digit headers)
        year_cols = [h for h in headers if re.match(r"^\d{4}$", h)]

        def add_rows(lst):
            for item in lst:
                data_row = item["data"][:]
                # Replace year-column "0" values with empty string
                for year in year_cols:
                    i = headers.index(year)
                    if i < len(data_row) and data_row[i] == "0":
                        data_row[i] = ""
                final_rows.append(data_row)
                bg = ["#ffffff"] * len(headers)
                if item.get("match") == "soft":
                    for i in range(len(headers)):
                        bg[i] = "#fff3b0"
                backgrounds.append(bg)

        add_rows(soft_matches)
        add_rows(others)

        # Clear sheet
        clear_sheet(spreadsheet_id, sheet_id)

        # Update values
        update_sheet_values(spreadsheet_id, sheet_name, final_rows)

        # Apply formatting
        set_sheet_formatting(
            spreadsheet_id=spreadsheet_id,
            sheet_id=sheet_id,
            header_row_count=1,
            total_rows=len(final_rows),
            total_cols=len(final_rows[0]),
            backgrounds=backgrounds,
        )

        # Rename file
        drive_service.files().update(
            fileId=spreadsheet_id, body={"name": "Complete Summary"}
        ).execute()
        config.logger.info("‚úèÔ∏è Renamed file to: Complete Summary")

        time.sleep(10)  # Sleep 10 seconds
        config.logger.info("‚úÖ Complete Summary deduplicated.")

    except Exception as e:
        config.logger.error(f"‚ùå Error processing Complete Summary: {e}")

    finally:
        helpers.release_folder_lock(config.SUMMARY_FOLDER_NAME)
        config.logger.info("üèÅ deduplicate_complete_summary complete.")


def deduplicate_rows_with_soft_match_complete_summary(sheet_data):
    dedup_fields = ["Title", "Remix", "Artist", "Comment", "Genre", "Year", "BPM"]
    desired_order = ["Title", "Remix", "Artist", "Comment", "Genre", "Year", "BPM", "Length"]

    all_headers = sheet_data[0]["header"]
    if not isinstance(all_headers, list):
        raise ValueError("Invalid allHeaders input ‚Äî expected an array.")

    lower_header_map = {h.lower(): h for h in all_headers}

    headers = [lower_header_map[h.lower()] for h in desired_order if h.lower() in lower_header_map]

    # Add year columns (e.g., 2016‚Äì2030+) detected in headers
    year_cols = [h for h in all_headers if re.match(r"^\d{4}$", h)]
    headers.extend([y for y in year_cols if y not in headers])

    primary_fields = [f for f in ["Title", "Remix", "Artist"] if f.lower() in lower_header_map]
    primary_fields = [lower_header_map[f.lower()] for f in primary_fields]
    secondary_fields = [
        f for f in dedup_fields if f not in primary_fields and f.lower() in lower_header_map
    ]
    secondary_fields = [lower_header_map[f.lower()] for f in secondary_fields]

    primary_indices = [{"field": f, "index": headers.index(f)} for f in primary_fields]
    secondary_indices = [{"field": f, "index": headers.index(f)} for f in secondary_fields]

    deduped_rows = []

    for sheet in sheet_data:
        header = sheet["header"]
        rows = sheet["rows"]
        header_map = {h.lower(): i for i, h in enumerate(header)}

        for row in rows:
            aligned = []
            for h in headers:
                idx = header_map.get(h.lower())
                if idx is not None and idx < len(row):
                    aligned.append(row[idx])
                else:
                    aligned.append("")
            # signature = "|".join(aligned[p["index"]] for p in primary_indices)

            matched = False
            for group in deduped_rows:
                other = group["data"]
                matches_all_primary = all(
                    aligned[p["index"]] == other[p["index"]] for p in primary_indices
                )
                if not matches_all_primary:
                    continue
                secondary_match = all(
                    (
                        not aligned[s["index"]]
                        or not other[s["index"]]
                        or aligned[s["index"]] == other[s["index"]]
                    )
                    for s in secondary_indices
                )
                if secondary_match:
                    # Merge year values (numeric sum)
                    for i, h in enumerate(headers):
                        if re.match(r"^\d{4}$", h):
                            try:
                                val_a = int(aligned[i]) if aligned[i] else 0
                            except ValueError:
                                val_a = 0
                            try:
                                val_b = int(other[i]) if other[i] else 0
                            except ValueError:
                                val_b = 0
                            other[i] = str(val_a + val_b)
                    group["count"] += 1
                    matched = True
                    break
            if not matched:
                deduped_rows.append({"data": aligned, "count": 1, "match": ""})

    # Soft match detection
    all_indices = primary_indices + secondary_indices
    for i in range(len(deduped_rows)):
        for j in range(i + 1, len(deduped_rows)):
            a = deduped_rows[i]
            b = deduped_rows[j]

            shared = get_shared_filled_fields(a["data"], b["data"], all_indices)
            score = get_dedup_match_score(a["data"], b["data"], all_indices)

            all_similar = all(
                (
                    not str(a["data"][idx["index"]])
                    or not str(b["data"][idx["index"]])
                    or string_similarity(
                        str(a["data"][idx["index"]]), str(b["data"][idx["index"]])
                    )
                    >= 0.5
                    or clean_title(str(a["data"][idx["index"]]))
                    == clean_title(str(b["data"][idx["index"]]))
                )
                for idx in all_indices
            )

            required_shared = 2 if len(primary_indices) >= 3 else 1
            if score >= 0.5 and shared >= required_shared and all_similar:
                if not a["match"]:
                    a["match"] = "soft"
                if not b["match"]:
                    b["match"] = "soft"

    return {"headers": headers, "rowsWithMeta": deduped_rows}
